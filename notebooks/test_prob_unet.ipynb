{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/15:   0%|▎                                                                                                                              | 1/343 [00:00<01:07,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:47<00:00,  3.19it/s, Loss: 0.6297]\n",
      ":: Evaluation :::   2%|██▊                                                                                                                                 | 2/92 [00:00<00:07, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.59it/s, Loss: 0.2374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 2/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:47<00:00,  3.19it/s, Loss: 0.2193]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:10<00:00,  9.20it/s, Loss: 0.2031]\n",
      "Train :: Epoch: 3/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.23it/s, Loss: 0.2013]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.76it/s, Loss: 0.1935]\n",
      "Train :: Epoch: 4/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.24it/s, Loss: 0.1940]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.78it/s, Loss: 0.1883]\n",
      "Train :: Epoch: 5/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:45<00:00,  3.24it/s, Loss: 0.1900]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.80it/s, Loss: 0.1850]\n",
      "Train :: Epoch: 6/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:45<00:00,  3.24it/s, Loss: 0.1865]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.75it/s, Loss: 0.1831]\n",
      "Train :: Epoch: 7/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:45<00:00,  3.24it/s, Loss: 0.1838]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.78it/s, Loss: 0.1818]\n",
      "Train :: Epoch: 8/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:45<00:00,  3.24it/s, Loss: 0.1820]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.72it/s, Loss: 0.1799]\n",
      "Train :: Epoch: 9/15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.23it/s, Loss: 0.1799]\n",
      ":: Evaluation ::: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.82it/s, Loss: 0.1793]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from dask.distributed import Client\n",
    "\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "from prob_unet_utils import plot_losses\n",
    "from accelerate import Accelerator\n",
    "import pickle\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # # Initialize Dask Client once\n",
    "    # client = Client()\n",
    "    # Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "\n",
    "    # Initializing the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=6,\n",
    "        num_filters=[64, 128, 256, 512],\n",
    "        beta=args.beta\n",
    "    ).to(args.device)\n",
    "\n",
    "    # Initializing the datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "\n",
    "    )\n",
    "    \n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # Initializing the dataloaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # Initializing training objects\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "    # Initialize loss tracking lists for each variable\n",
    "    tr_losses_mae = {var: [] for var in args.variables}\n",
    "    tr_losses_kl = {var: [] for var in args.variables}\n",
    "    val_losses_mae = {var: [] for var in args.variables}\n",
    "    val_losses_kl = {var: [] for var in args.variables}\n",
    "\n",
    "    # Initialize loss storage dictionaries\n",
    "    all_train_losses_mae = {var: [] for var in args.variables}\n",
    "    all_train_losses_kl = {var: [] for var in args.variables}\n",
    "    all_val_losses_mae = {var: [] for var in args.variables}\n",
    "    all_val_losses_kl = {var: [] for var in args.variables}\n",
    "\n",
    "    # initial_beta = 0\n",
    "    # max_beta = args.beta\n",
    "    # num_warmup_epochs = 5\n",
    "    # args.num_epochs = 30\n",
    "    \n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        \n",
    "        # if epoch <= num_warmup_epochs:\n",
    "        #     # Keep beta at zero for the warmup period\n",
    "        #     current_beta = 0\n",
    "        # else:\n",
    "        #     # Gradually increase beta from 0 to max_beta over the remaining epochs\n",
    "        #     progress = (epoch - num_warmup_epochs) / (args.num_epochs - num_warmup_epochs)\n",
    "        #     current_beta = progress * max_beta\n",
    "        # # Gradually increase beta\n",
    "        # # current_beta = min(initial_beta + epoch * (max_beta / args.num_epochs), max_beta)\n",
    "\n",
    "        # # Ensure current_beta does not exceed max_beta\n",
    "        # current_beta = min(current_beta, max_beta)\n",
    "        \n",
    "        # # Set the current beta for the model\n",
    "        # probunet_model.beta = current_beta\n",
    "\n",
    "        # Training for one epoch\n",
    "        train_losses_mae, training_losses_kl = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,\n",
    "            variables=args.variables,\n",
    "        )\n",
    "        for var in args.variables:\n",
    "            tr_losses_mae[var].append(train_losses_mae[var])\n",
    "            tr_losses_kl[var].append(training_losses_kl[var])\n",
    "            # Store losses in epoch dictionaries\n",
    "            all_train_losses_mae[var].append(train_losses_mae[var])\n",
    "            all_train_losses_kl[var].append(training_losses_kl[var])\n",
    "\n",
    "        # Evaluating the model on validation data\n",
    "        val_losses_mae_running, val_losses_kl_running = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            reconstruct=False,\n",
    "            device=args.device,           \n",
    "        )\n",
    "        for var in args.variables:\n",
    "            val_losses_mae[var].append(val_losses_mae_running[var])\n",
    "            val_losses_kl[var].append(val_losses_kl_running[var])\n",
    "\n",
    "            # Store losses in epoch dictionaries\n",
    "            all_val_losses_mae[var].append(val_losses_mae_running[var])\n",
    "            all_val_losses_kl[var].append(val_losses_kl_running[var])\n",
    "\n",
    "\n",
    "        # Sampling from the model every 2 epochs\n",
    "        # if epoch % 2 == 0:\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device\n",
    "        )\n",
    "        # Save sample plots\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "    \n",
    "        # Save losses to a file after training\n",
    "    losses_to_save = {\n",
    "        \"train_losses_mae\": all_train_losses_mae,\n",
    "        \"train_losses_kl\": all_train_losses_kl,\n",
    "        \"val_losses_mae\": all_val_losses_mae,\n",
    "        \"val_losses_kl\": all_val_losses_kl,\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "\n",
    "    # Plot training and validation loss curves for each variable\n",
    "    plot_losses(tr_losses_mae, tr_losses_kl, val_losses_mae, val_losses_kl, args.variables, args.plotdir)\n",
    "\n",
    "    # # Close the Dask Client after training is complete\n",
    "    # client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = tm.get_args()\n",
    "# Initializing the Probabilistic UNet model\n",
    "probunet_model = ProbabilisticUNet(\n",
    "    input_channels=len(args.variables),\n",
    "    num_classes=len(args.variables),\n",
    "    latent_dim=6,\n",
    "    num_filters=[64, 128, 256, 512],\n",
    "    beta=args.beta\n",
    ").to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_prob = sum(\n",
    "param.numel() for param in probunet_model.parameters()\n",
    ")\n",
    "total_params_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import UNet\n",
    "\n",
    "model_0 = UNet(\n",
    "            img_resolution=(64, 64),  \n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            label_dim=0,\n",
    "            use_diffuse=False\n",
    "        )\n",
    "\n",
    "total_params_unet = sum(param.numel() for param in model_0.parameters())\n",
    "total_params_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_prob - total_params_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
